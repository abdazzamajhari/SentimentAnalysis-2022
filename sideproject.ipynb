{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31ce6c3e",
   "metadata": {},
   "source": [
    "## Pengaruh Kelangkaan Minyak Goreng Tahun 2022 Terkait Kepercayaan Masyarakat Terhadap Kinerja Pemerintah\n",
    "\n",
    "https://yunusmuhammad007.medium.com/text-preprocessing-menggunakan-pandas-nltk-dan-sastrawi-untuk-large-dataset-5fb3c0a88571"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3188e616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweepy in c:\\anaconda3\\envs\\notebook\\lib\\site-packages (4.7.0)\n",
      "Requirement already satisfied: oauthlib<4,>=3.2.0 in c:\\anaconda3\\envs\\notebook\\lib\\site-packages (from tweepy) (3.2.0)\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in c:\\anaconda3\\envs\\notebook\\lib\\site-packages (from tweepy) (2.27.1)\n",
      "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in c:\\anaconda3\\envs\\notebook\\lib\\site-packages (from tweepy) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\anaconda3\\envs\\notebook\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda3\\envs\\notebook\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\anaconda3\\envs\\notebook\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\anaconda3\\envs\\notebook\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2.0.12)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6514e783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\anaconda3\\envs\\notebook\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: click in c:\\anaconda3\\envs\\notebook\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\anaconda3\\envs\\notebook\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\anaconda3\\envs\\notebook\\lib\\site-packages (from nltk) (4.63.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\anaconda3\\envs\\notebook\\lib\\site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: colorama in c:\\anaconda3\\envs\\notebook\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "300cfb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in c:\\anaconda3\\envs\\notebook\\lib\\site-packages (3.0.9)\n",
      "Requirement already satisfied: et-xmlfile in c:\\anaconda3\\envs\\notebook\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c3eb8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.11.0\n",
      "  latest version: 4.12.0\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c440d6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\H4WK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\H4WK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import tweepy as tw\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import time\n",
    "import string\n",
    "# import word_tokenize & FreqDist from NLTK\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e92f666",
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = '1342392531359682560-D7JBU2jigpySHc5W9o3jdCVK6jLMDo'\n",
    "access_token_secret = 'VOx7W6KTs7PrKk6afLuv9orOeaRs8ujIjVgutCNysDG9j'\n",
    "consumer_key = 'oxxQTv6HRJV9oRkYKmEVZ6c5f'\n",
    "consumer_secret = 'Trf22Tjqncw8j3l8FD1f02p44n0Pjqh2Fhbrv7XMwR5mpz3YT5'\n",
    "\n",
    "#bearer_token = 'AAAAAAAAAAAAAAAAAAAAAD%2FcKwEAAAAAabCoEW3Jc%2BU6yL2n9GVvGn9OxE0%3DQIROzH5vjm1uyFT0VidYBP6XcgB5BNz3KMJzWHL9qxB2q10rZr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db28c193",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tw.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a2d2358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the search term and the date_since date as variables\n",
    "search_words = ['minyak goreng', 'langka', 'pemerintah']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52f547c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tw.Cursor(api.search_tweets, q=search_words, lang=\"id\").items(1000)\n",
    "# tweets = tw.Cursor(api.search_tweets, q=search_words, lang=\"id\").items()\n",
    "\n",
    "users_locs = [[tweet.user.screen_name, tweet.user.location, tweet.text] for tweet in tweets]\n",
    "# users_locs #matiin dulu biar ga kebanyakan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75656638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>location</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EthanaelGiovan</td>\n",
       "      <td></td>\n",
       "      <td>RT @Naj_2709: Amin. Semoga di bulan Ramadhan m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AsbiansaAbi</td>\n",
       "      <td>Kolong</td>\n",
       "      <td>Kemarin waktu minyak goreng langka. saya dari ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hasanudin</td>\n",
       "      <td>indramayu</td>\n",
       "      <td>RT @TeddGus: Misalnya di Minimarket X, kemarin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AyunHamunta</td>\n",
       "      <td></td>\n",
       "      <td>Puan Murka Harga Minyak Goreng Belum Normal: I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fahri_nusantara</td>\n",
       "      <td>Kota Parepare</td>\n",
       "      <td>RT @voidotid: Sudah Dijamin Dapat Subsidi dari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>AmirGrage</td>\n",
       "      <td></td>\n",
       "      <td>RT @PKSejahtera: Pemerintah Harus Selesaikan K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>rioprosetara</td>\n",
       "      <td>DKI Jakarta, Indonesia</td>\n",
       "      <td>@Mbezz_Syalala ya uda gitu nyinyir sm pemerint...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Konde_001</td>\n",
       "      <td>+62</td>\n",
       "      <td>RT @PKSejahtera: Pemerintah Harus Selesaikan K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>mulyatikaral</td>\n",
       "      <td></td>\n",
       "      <td>RT @PKSejahtera: Pemerintah Harus Selesaikan K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>pkspaser</td>\n",
       "      <td></td>\n",
       "      <td>RT @PKSejahtera: Pemerintah Harus Selesaikan K...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                user                location  \\\n",
       "0     EthanaelGiovan                           \n",
       "1        AsbiansaAbi                  Kolong   \n",
       "2          Hasanudin               indramayu   \n",
       "3        AyunHamunta                           \n",
       "4    fahri_nusantara           Kota Parepare   \n",
       "..               ...                     ...   \n",
       "995        AmirGrage                           \n",
       "996     rioprosetara  DKI Jakarta, Indonesia   \n",
       "997        Konde_001                     +62   \n",
       "998     mulyatikaral                           \n",
       "999         pkspaser                           \n",
       "\n",
       "                                                 tweet  \n",
       "0    RT @Naj_2709: Amin. Semoga di bulan Ramadhan m...  \n",
       "1    Kemarin waktu minyak goreng langka. saya dari ...  \n",
       "2    RT @TeddGus: Misalnya di Minimarket X, kemarin...  \n",
       "3    Puan Murka Harga Minyak Goreng Belum Normal: I...  \n",
       "4    RT @voidotid: Sudah Dijamin Dapat Subsidi dari...  \n",
       "..                                                 ...  \n",
       "995  RT @PKSejahtera: Pemerintah Harus Selesaikan K...  \n",
       "996  @Mbezz_Syalala ya uda gitu nyinyir sm pemerint...  \n",
       "997  RT @PKSejahtera: Pemerintah Harus Selesaikan K...  \n",
       "998  RT @PKSejahtera: Pemerintah Harus Selesaikan K...  \n",
       "999  RT @PKSejahtera: Pemerintah Harus Selesaikan K...  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data=users_locs, columns=['user', 'location', 'tweet'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "509378f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cleanTxt(text):\n",
    "#     text = re.sub('@[A-Za-z0–9]+', '', text) # Menghapus @mentions\n",
    "#     text = re.sub('#', '', text) # Menghapus '#' hash tag\n",
    "#     text = re.sub('_', '', text) # Menghapus '_' underscore\n",
    "#     text = re.sub(':', '', text) # Menghapus ':' \n",
    "#     text = re.sub('RT[\\s]+', '', text) # Menghapus RT\n",
    "#     text = re.sub('https?:\\/\\/\\S+', '', text) # Menghapus hyperlink\n",
    "#     return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7b8f5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['tweet'] = df['tweet'].apply(cleanTxt)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2a8012c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['tweet'] = df['tweet'].drop_duplicates()\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2fb9875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #drop NaN data\n",
    "# df = df.dropna()\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed77d764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_noise(tweet, stop_words = ()):\n",
    "\n",
    "#     cleaned_tokens = []\n",
    "\n",
    "#     for token, tag in pos_tag(tweets):\n",
    "#         token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "#                        '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "#         token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "#         if tag.startswith(\"NN\"):\n",
    "#             pos = 'n'\n",
    "#         elif tag.startswith('VB'):\n",
    "#             pos = 'v'\n",
    "#         else:\n",
    "#             pos = 'a'\n",
    "\n",
    "#         lemmatizer = WordNetLemmatizer()\n",
    "#         token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "#         if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "#             cleaned_tokens.append(token.lower())\n",
    "#     return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5c1782a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ------ Tokenizing ---------\n",
    "\n",
    "# def remove_tweet_special(text):\n",
    "#     # remove tab, new line, ans back slice\n",
    "#     text = text.replace('\\\\t',\" \").replace('\\\\n',\" \").replace('\\\\u',\" \").replace('\\\\',\"\")\n",
    "#     # remove non ASCII (emoticon, chinese word, .etc)\n",
    "#     text = text.encode('ascii', 'replace').decode('ascii')\n",
    "#     # remove mention, link, hashtag\n",
    "#     text = ' '.join(re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\",\" \", text).split())\n",
    "#     # remove incomplete URL\n",
    "#     return text.replace(\"http://\", \" \").replace(\"https://\", \" \")\n",
    "                \n",
    "# df['tweet'] = df['tweet'].apply(remove_tweet_special)\n",
    "\n",
    "# #remove number\n",
    "# def remove_number(text):\n",
    "#     return  re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "# df['tweet'] = df['tweet'].apply(remove_number)\n",
    "\n",
    "# #remove punctuation\n",
    "# def remove_punctuation(text):\n",
    "#     return text.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "\n",
    "# df['tweet'] = df['tweet'].apply(remove_punctuation)\n",
    "\n",
    "# #remove whitespace leading & trailing\n",
    "# def remove_whitespace_LT(text):\n",
    "#     return text.strip()\n",
    "\n",
    "# df['tweet'] = df['tweet'].apply(remove_whitespace_LT)\n",
    "\n",
    "# #remove multiple whitespace into single whitespace\n",
    "# def remove_whitespace_multiple(text):\n",
    "#     return re.sub('\\s+',' ',text)\n",
    "\n",
    "# df['tweet'] = df['tweet'].apply(remove_whitespace_multiple)\n",
    "\n",
    "# # remove single char\n",
    "# def remove_singl_char(text):\n",
    "#     return re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "\n",
    "# df['tweet'] = df['tweet'].apply(remove_singl_char)\n",
    "\n",
    "# # NLTK word rokenize \n",
    "# def word_tokenize_wrapper(text):\n",
    "#     return word_tokenize(text)\n",
    "\n",
    "# df['tweet_tokens'] = df['tweet'].apply(word_tokenize_wrapper)\n",
    "\n",
    "# print('Tokenizing Result : \\n') \n",
    "# print(df['tweet_tokens'].head())\n",
    "# print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "663f95b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NLTK calc frequency distribution\n",
    "# def freqDist_wrapper(text):\n",
    "#     return FreqDist(text)\n",
    "\n",
    "# df['tweet_tokens_fdist'] = df['tweet_tokens'].apply(freqDist_wrapper)\n",
    "\n",
    "# print('Frequency Tokens : \\n') \n",
    "# print(df['tweet_tokens_fdist'].head().apply(lambda x : x.most_common()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68cf4107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ----------------------- get stopword from NLTK stopword -------------------------------\n",
    "# # get stopword indonesia\n",
    "# list_stopwords = stopwords.words('indonesian')\n",
    "\n",
    "\n",
    "# # ---------------------------- manualy add stopword  ------------------------------------\n",
    "# # append additional stopword\n",
    "# list_stopwords.extend(['amp', 'rt', '&amp'])\n",
    "\n",
    "# # ----------------------- add stopword from txt file ------------------------------------\n",
    "# # read txt stopword using pandas\n",
    "# txt_stopword = pd.read_csv(\"stopwords.txt\", names= [\"stopwords\"], header = None)\n",
    "\n",
    "# # convert stopword string to list & append additional stopword\n",
    "# list_stopwords.extend(txt_stopword[\"stopwords\"][0].split(' '))\n",
    "\n",
    "# # ---------------------------------------------------------------------------------------\n",
    "\n",
    "# # convert list to dictionary\n",
    "# list_stopwords = set(list_stopwords)\n",
    "\n",
    "\n",
    "# #remove stopword pada list token\n",
    "# def stopwords_removal(words):\n",
    "#     return [word for word in words if word not in list_stopwords]\n",
    "\n",
    "# df['tweet_tokens_WSW'] = df['tweet_tokens'].apply(stopwords_removal) \n",
    "\n",
    "\n",
    "# print(df['tweet_tokens_WSW'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea2eaf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b38595d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized_word = pd.read_excel(\"normalisasi.xlsx\")\n",
    "\n",
    "# normalized_word_dict = {}\n",
    "\n",
    "# for index, row in normalized_word.iterrows():\n",
    "#     if row[0] not in normalized_word_dict:\n",
    "#         normalized_word_dict[row[0]] = row[1] \n",
    "\n",
    "# def normalized_term(document):\n",
    "#     return [normalized_word_dict[term] if term in normalized_word_dict else term for term in document]\n",
    "\n",
    "# df['tweet_normalized'] = df['tweet_tokens_WSW'].apply(normalized_term)\n",
    "\n",
    "# df['tweet_normalized'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0ef95f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import Sastrawi package\n",
    "# from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "# import swifter\n",
    "\n",
    "\n",
    "# # create stemmer\n",
    "# factory = StemmerFactory()\n",
    "# stemmer = factory.create_stemmer()\n",
    "\n",
    "# # stemmed\n",
    "# def stemmed_wrapper(term):\n",
    "#     return stemmer.stem(term)\n",
    "\n",
    "# term_dict = {}\n",
    "\n",
    "# for document in df['tweet_normalized']:\n",
    "#     for term in document:\n",
    "#         if term not in term_dict:\n",
    "#             term_dict[term] = ' '\n",
    "            \n",
    "# print(len(term_dict))\n",
    "# print(\"------------------------\")\n",
    "\n",
    "# for term in term_dict:\n",
    "#     term_dict[term] = stemmed_wrapper(term)\n",
    "#     print(term,\":\" ,term_dict[term])\n",
    "    \n",
    "# print(term_dict)\n",
    "# print(\"------------------------\")\n",
    "\n",
    "\n",
    "# # apply stemmed term to dataframe\n",
    "# def get_stemmed_term(document):\n",
    "#     return [term_dict[term] for term in document]\n",
    "\n",
    "# df['tweet_tokens_stemmed'] = df['tweet_normalized'].swifter.apply(get_stemmed_term)\n",
    "# print(df['tweet_tokens_stemmed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba77ae85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd0ba6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.reset_index(drop=True, inplace=True)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1fc55a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save ke CSV\n",
    "df.to_csv(\"dataset_19maret.csv\")\n",
    "\n",
    "#Save ke Excel\n",
    "# df.to_excel(\"Text_Preprocessing.xlsx\")\n",
    "\n",
    "#Save ke Binary HDF5\n",
    "# df.to_hdf(\"Text_Preprocessing.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "956ec781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def readwords( filename ):\n",
    "#     f = open(filename)\n",
    "#     words = [ line.rstrip() for line in f.readlines()]\n",
    "#     return words\n",
    "\n",
    "# def FilterList():\n",
    "#     sentence = df['tweet'][1]\n",
    "#     #tokens=Token()\n",
    "#     #print (\"menghitung jumlah kata negatif dan positif\\n.\\n.\\n.\")\n",
    "#     positive = readwords('positive.txt')\n",
    "#     negative = readwords('negative.txt')\n",
    "#     paragraph = str(sentence)\n",
    "#     token = word_tokenize(paragraph)\n",
    "#     token = [w.lower() for w in token]\n",
    "#     table = str.maketrans('', '', string.punctuation)\n",
    "#     stripped = [w.translate(table) for w in token]\n",
    "#     words = [word for word in stripped if word.isalpha()]\n",
    "#     kata = str(words)\n",
    "#     detoken = re.sub(\"', '\", ' ', kata)\n",
    "#     count = Counter(detoken.split())\n",
    "#     #print (\"---> membaca list kalimat positif dan negatif\\n.\\n.\\n.\")\n",
    "#     pos = 0\n",
    "#     neg = 0\n",
    "#     for key, val in count.items():\n",
    "#         key = key.rstrip('.,?!\\n') # removing possible punctuation signs\n",
    "#         if key in positive:\n",
    "#             pos += val\n",
    "#             f4= open(\"listpositif\",\"a+\")\n",
    "#             f4.write(str(key)+\",\"+str(val)+\"\\r\")\n",
    "#             f4.close()\n",
    "#         if key in negative:\n",
    "#             neg += val\n",
    "#             f5= open(\"listnegatif\",\"a+\")\n",
    "#             f5.write(str(key)+\",\"+str(val)+\"\\r\")\n",
    "#             f5.close()\n",
    "#     a=str(pos)\n",
    "#     b=str(neg)\n",
    "#     f2= open(\"positif\",\"a+\")\n",
    "#     f2.write(a+\"\\r\")\n",
    "#     f2.close()\n",
    "#     f3= open(\"negatif\",\"a+\")\n",
    "#     f3.write(b+\"\\r\")\n",
    "#     f3.close()\n",
    "#     hasil1 = (\"jumlah positif     : \"+a)\n",
    "#     hasil2 = (\"cenderung negatif  : \"+b)\n",
    "#     return (a, b)\n",
    "\n",
    "# a,b=FilterList()\n",
    "# print(\"jumlah positif  : \"+a)\n",
    "# print(\"jumlah negatif  : \"+b+\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935e901e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
